{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbcf794c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "# lets import some stuff\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import Constant\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re\n",
    "from keras import regularizers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "925ae8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2060\n"
     ]
    }
   ],
   "source": [
    "# confirm  the GPU\n",
    "from torch import cuda\n",
    "assert cuda.is_available()\n",
    "assert cuda.device_count() > 0\n",
    "print(cuda.get_device_name(cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa2b5390",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000 # this is the number of words we care about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80dfff66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>section_nr</th>\n",
       "      <th>last_section_title</th>\n",
       "      <th>Labels</th>\n",
       "      <th>has_citation</th>\n",
       "      <th>Labels_rule</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This live defined as 1 00 Defination Defination</td>\n",
       "      <td>1</td>\n",
       "      <td>Defination</td>\n",
       "      <td>Defination</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Defination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A sunshine be a maven that shine 1 00 Definati...</td>\n",
       "      <td>1</td>\n",
       "      <td>Defination</td>\n",
       "      <td>Defination</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Defination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e set galaxy as the group of star 1 00 Definat...</td>\n",
       "      <td>1</td>\n",
       "      <td>Defination</td>\n",
       "      <td>Defination</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Defination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e ask that these were in all likeliness stimul...</td>\n",
       "      <td>1</td>\n",
       "      <td>Hypothesis</td>\n",
       "      <td>Hypothesis</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Hypothesis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Information technology could trace theorise th...</td>\n",
       "      <td>4</td>\n",
       "      <td>Hypothesis</td>\n",
       "      <td>Hypothesis</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Hypothesis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18147</th>\n",
       "      <td>we therefore conclude that the issue of experi...</td>\n",
       "      <td>70</td>\n",
       "      <td>functional magnetic resonance imaging data ana...</td>\n",
       "      <td>Emperical Result</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Emperical Result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18149</th>\n",
       "      <td>these new insight we discovered open possiblen...</td>\n",
       "      <td>80</td>\n",
       "      <td>contributions to research</td>\n",
       "      <td>Future work</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Contribution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18153</th>\n",
       "      <td>as users tend to behave impulsively with mobil...</td>\n",
       "      <td>87</td>\n",
       "      <td>limitations and future topics</td>\n",
       "      <td>limitation</td>\n",
       "      <td>1.0</td>\n",
       "      <td>limitation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18154</th>\n",
       "      <td>tertiary using yes  no resolution choice for e...</td>\n",
       "      <td>87</td>\n",
       "      <td>limitations and future topics</td>\n",
       "      <td>limitation</td>\n",
       "      <td>0.0</td>\n",
       "      <td>limitation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18155</th>\n",
       "      <td>fifth our report centre on highly negative inc...</td>\n",
       "      <td>87</td>\n",
       "      <td>limitations and future topics</td>\n",
       "      <td>limitation</td>\n",
       "      <td>1.0</td>\n",
       "      <td>limitation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11966 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  section_nr  \\\n",
       "0        This live defined as 1 00 Defination Defination           1   \n",
       "1      A sunshine be a maven that shine 1 00 Definati...           1   \n",
       "2      e set galaxy as the group of star 1 00 Definat...           1   \n",
       "3      e ask that these were in all likeliness stimul...           1   \n",
       "4      Information technology could trace theorise th...           4   \n",
       "...                                                  ...         ...   \n",
       "18147  we therefore conclude that the issue of experi...          70   \n",
       "18149  these new insight we discovered open possiblen...          80   \n",
       "18153  as users tend to behave impulsively with mobil...          87   \n",
       "18154  tertiary using yes  no resolution choice for e...          87   \n",
       "18155  fifth our report centre on highly negative inc...          87   \n",
       "\n",
       "                                      last_section_title            Labels  \\\n",
       "0                                             Defination        Defination   \n",
       "1                                             Defination        Defination   \n",
       "2                                             Defination        Defination   \n",
       "3                                             Hypothesis        Hypothesis   \n",
       "4                                             Hypothesis        Hypothesis   \n",
       "...                                                  ...               ...   \n",
       "18147  functional magnetic resonance imaging data ana...  Emperical Result   \n",
       "18149                         contributions to research        Future work   \n",
       "18153                     limitations and future topics         limitation   \n",
       "18154                     limitations and future topics         limitation   \n",
       "18155                     limitations and future topics         limitation   \n",
       "\n",
       "       has_citation       Labels_rule  \n",
       "0               0.0        Defination  \n",
       "1               0.0        Defination  \n",
       "2               0.0        Defination  \n",
       "3               0.0        Hypothesis  \n",
       "4               0.0        Hypothesis  \n",
       "...             ...               ...  \n",
       "18147           0.0  Emperical Result  \n",
       "18149           1.0      Contribution  \n",
       "18153           1.0        limitation  \n",
       "18154           0.0        limitation  \n",
       "18155           1.0        limitation  \n",
       "\n",
       "[11966 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read training dataset with rule based feature\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\nitis\\Sentence classification\\theis_final\\rule_train_data.csv', index_col=0)\n",
    "df = df.drop_duplicates(subset='sentence', keep=\"first\")\n",
    "#df = df[1:100]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f7c6a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean length of sentence: 29.011783386261072\n",
      "max length of sentence: 142\n",
      "std dev length of sentence: 16.1405738044385\n"
     ]
    }
   ],
   "source": [
    "df['l'] = df['sentence'].apply(lambda x: len(str(x).split(' ')))\n",
    "print(\"mean length of sentence: \" + str(df.l.mean()))\n",
    "print(\"max length of sentence: \" + str(df.l.max()))\n",
    "print(\"std dev length of sentence: \" + str(df.l.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "103d7c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these sentences aren't that long so we may as well use the whole string\n",
    "sequence_length = 142"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fd70a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>section_nr</th>\n",
       "      <th>has_citation</th>\n",
       "      <th>last_section_title</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The aim of this thesis was to gain an understa...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Research Aim</td>\n",
       "      <td>Aim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The aim is to develop finite element models us...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Research Objective</td>\n",
       "      <td>Aim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Determine the relationship between the size of...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Research Objective</td>\n",
       "      <td>Aim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Investigate the influence of nonuniform cup su...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Research Objective</td>\n",
       "      <td>Aim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Examine the influence of errors during reaming...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Research Objective</td>\n",
       "      <td>Aim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>the moment of enjoyment on the pattern to rais...</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>hypothesis outcome</td>\n",
       "      <td>Hypothesis Result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>Information technology substantiate the hypoth...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>hypothesis outcome</td>\n",
       "      <td>Hypothesis Result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>information engineering science rejects the su...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>hypothesis outcome</td>\n",
       "      <td>Hypothesis Result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>the supposition stern follow agreed on the cor...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>hypothesis outcome</td>\n",
       "      <td>Hypothesis Result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>the final result prove the possibility that th...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>hypothesis outcome</td>\n",
       "      <td>Hypothesis Result</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>582 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence  section_nr  \\\n",
       "0    The aim of this thesis was to gain an understa...           1   \n",
       "1    The aim is to develop finite element models us...           1   \n",
       "2    Determine the relationship between the size of...           1   \n",
       "3    Investigate the influence of nonuniform cup su...           1   \n",
       "4    Examine the influence of errors during reaming...           1   \n",
       "..                                                 ...         ...   \n",
       "577  the moment of enjoyment on the pattern to rais...          39   \n",
       "578  Information technology substantiate the hypoth...          10   \n",
       "579  information engineering science rejects the su...          10   \n",
       "580  the supposition stern follow agreed on the cor...          10   \n",
       "581  the final result prove the possibility that th...          10   \n",
       "\n",
       "     has_citation   last_section_title             Labels  \n",
       "0               0         Research Aim                Aim  \n",
       "1               0   Research Objective                Aim  \n",
       "2               0   Research Objective                Aim  \n",
       "3               0   Research Objective                Aim  \n",
       "4               0   Research Objective                Aim  \n",
       "..            ...                  ...                ...  \n",
       "577             0  hypothesis outcome   Hypothesis Result  \n",
       "578             0  hypothesis outcome   Hypothesis Result  \n",
       "579             0  hypothesis outcome   Hypothesis Result  \n",
       "580             0  hypothesis outcome   Hypothesis Result  \n",
       "581             0  hypothesis outcome   Hypothesis Result  \n",
       "\n",
       "[582 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the testing dataset with rule based features\n",
    "cols = ['sentence', 'section_nr','has_citation','last_section_title','Labels']\n",
    "cols1 = ['sentence', 'section_nr','has_citation','last_section_title']\n",
    "test = pd.read_csv(r'C:\\Users\\nitis\\Sentence classification\\theis_final\\rule_test_data.csv')\n",
    "test = test[cols]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37723b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train = df[\"sentence\"].values\n",
    "sentences_test = test[\"sentence\"].values\n",
    "\n",
    "y_train =  pd.get_dummies(df['Labels']).values\n",
    "y_test  = pd.get_dummies(test['Labels']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bc10407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e set galaxy as the group of star 1 00 Defination Defination\n",
      "[174, 214, 4203, 16, 1, 253, 3, 1234, 11, 2, 72, 72]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(sentences_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
    "X_test = tokenizer.texts_to_sequences(sentences_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "\n",
    "print(sentences_train[2])\n",
    "print(X_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00723eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 17 350 537  16  11   2  72  72   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "X_train = pad_sequences(X_train, padding='post', maxlen=sequence_length)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=sequence_length)\n",
    "\n",
    "print(X_train[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b3c34b",
   "metadata": {},
   "source": [
    "#  Model 1: Random embeddings\n",
    "\n",
    "Lets build our model. In general I'm going to just use the same hyperparameters as Kim does (see section 3.1 of his paper) apart from the embedding dimension\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a3f0e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 200 # Kim uses 300 here\n",
    "num_filters = 100\n",
    "\n",
    "inputs = Input(shape=(sequence_length,), dtype='int32')\n",
    "\n",
    "# use a random embedding for the text\n",
    "embedding_layer = Embedding(input_dim=max_features, output_dim=embedding_dim, input_length=sequence_length)(inputs)\n",
    "\n",
    "reshape = Reshape((sequence_length, embedding_dim, 1))(embedding_layer)\n",
    "\n",
    "# Note the relu activation which Kim specifically mentions\n",
    "# He also uses an l2 constraint of 3\n",
    "# Also, note that the convolution window acts on the whole 200 dimensions - that's important\n",
    "conv_0 = Conv2D(num_filters, kernel_size=(3, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape)\n",
    "conv_1 = Conv2D(num_filters, kernel_size=(4, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape)\n",
    "conv_2 = Conv2D(num_filters, kernel_size=(5, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape)\n",
    "\n",
    "# perform max pooling on each of the convoluations\n",
    "maxpool_0 = MaxPool2D(pool_size=(sequence_length - 3 + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
    "maxpool_1 = MaxPool2D(pool_size=(sequence_length - 4 + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
    "maxpool_2 = MaxPool2D(pool_size=(sequence_length - 5 + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
    "\n",
    "# concat and flatten\n",
    "concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
    "flatten = Flatten()(concatenated_tensor)\n",
    "\n",
    "# do dropout and predict\n",
    "dropout = Dropout(0.5)(flatten)\n",
    "output = Dense(units=12, activation='softmax')(dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f93b6f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 142)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 142, 200)     2000000     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 142, 200, 1)  0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 140, 1, 100)  60100       ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 139, 1, 100)  80100       ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 138, 1, 100)  100100      ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 1, 1, 100)    0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 100)   0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 100)   0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 3, 1, 100)    0           ['max_pooling2d[0][0]',          \n",
      "                                                                  'max_pooling2d_1[0][0]',        \n",
      "                                                                  'max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 300)          0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 300)          0           ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 12)           3612        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,243,912\n",
      "Trainable params: 2,243,912\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=inputs, outputs=output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65bd8e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "216/216 [==============================] - 25s 36ms/step - loss: 2.7795 - accuracy: 0.1467 - val_loss: 2.5886 - val_accuracy: 0.2264\n",
      "Epoch 2/5\n",
      "216/216 [==============================] - 3s 12ms/step - loss: 1.9976 - accuracy: 0.3960 - val_loss: 2.0673 - val_accuracy: 0.3275\n",
      "Epoch 3/5\n",
      "216/216 [==============================] - 3s 12ms/step - loss: 1.6746 - accuracy: 0.5513 - val_loss: 1.9349 - val_accuracy: 0.4336\n",
      "Epoch 4/5\n",
      "216/216 [==============================] - 3s 12ms/step - loss: 1.5248 - accuracy: 0.6782 - val_loss: 1.8396 - val_accuracy: 0.4912\n",
      "Epoch 5/5\n",
      "216/216 [==============================] - 3s 12ms/step - loss: 1.3741 - accuracy: 0.7598 - val_loss: 1.6150 - val_accuracy: 0.5063\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50 # Kim uses 50 here, I have a slightly smaller sample size than num\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=batch_size, verbose=1, validation_split=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ed2dd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c759e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.520618556701031"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b341bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6,  0,  7,  0,  0,  0,  0,  0,  0,  0,  5,  0],\n",
       "       [ 0, 56,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0],\n",
       "       [ 0,  3,  6,  0,  0,  0,  0,  0,  9,  0,  7,  0],\n",
       "       [ 0,  0,  3,  3,  2,  0,  0,  3,  0,  1,  0, 34],\n",
       "       [ 0,  2, 14,  0, 69,  2,  0,  2,  0,  6,  1,  0],\n",
       "       [ 0,  0,  1,  2,  0, 42,  0,  1,  1,  0, 23,  0],\n",
       "       [ 0,  0,  4,  2,  1, 31,  0,  0,  0,  0,  0,  5],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 42,  0,  0,  1,  0],\n",
       "       [ 1,  0,  3,  0,  0,  0,  0,  0,  9,  3,  8,  0],\n",
       "       [ 0,  0,  6,  0, 30,  0,  0,  6,  3,  3,  6,  0],\n",
       "       [ 0,  0,  0,  2,  0,  0,  0,  0,  0,  0, 65,  0],\n",
       "       [ 0,  0,  4,  0, 30,  1,  0,  1,  0,  0,  1,  2]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6125d3",
   "metadata": {},
   "source": [
    "#  Model 2: Static word2vec\n",
    "Now rather than randomly assign vectors we're going use w2v embeddings. This took me quite a long time to get right, so I'll walk through it line by line\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1af0bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join('C:\\\\Users\\\\nitis\\\\OneDrive\\\\Desktop\\\\thesis', 'glove.6B.200d.txt'), encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74a77f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10960 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b5c9222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10001\n"
     ]
    }
   ],
   "source": [
    "num_words = min(max_features, len(word_index)) + 1\n",
    "print(num_words)\n",
    "\n",
    "# first create a matrix of zeros, this is our embedding matrix\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "# for each word in out tokenizer lets try to find that work in our w2v model\n",
    "for word, i in word_index.items():\n",
    "    if i > max_features:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # we found the word - add that words vector to the matrix\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        # doesn't exist, assign a random vector\n",
    "        embedding_matrix[i] = np.random.randn(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9809347",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_2 = Input(shape=(sequence_length,), dtype='int32')\n",
    "\n",
    "# note the `trainable=False`, later we will make this layer trainable\n",
    "embedding_layer_2 = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=sequence_length,\n",
    "                            trainable=False)(inputs_2)\n",
    "\n",
    "reshape_2 = Reshape((sequence_length, embedding_dim, 1))(embedding_layer_2)\n",
    "\n",
    "conv_0_2 = Conv2D(num_filters, kernel_size=(3, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_2)\n",
    "conv_1_2 = Conv2D(num_filters, kernel_size=(4, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_2)\n",
    "conv_2_2 = Conv2D(num_filters, kernel_size=(5, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_2)\n",
    "\n",
    "maxpool_0_2 = MaxPool2D(pool_size=(sequence_length - 3 + 1, 1), strides=(1,1), padding='valid')(conv_0_2)\n",
    "maxpool_1_2 = MaxPool2D(pool_size=(sequence_length - 4 + 1, 1), strides=(1,1), padding='valid')(conv_1_2)\n",
    "maxpool_2_2 = MaxPool2D(pool_size=(sequence_length - 5 + 1, 1), strides=(1,1), padding='valid')(conv_2_2)\n",
    "\n",
    "concatenated_tensor_2 = Concatenate(axis=1)([maxpool_0_2, maxpool_1_2, maxpool_2_2])\n",
    "flatten_2 = Flatten()(concatenated_tensor_2)\n",
    "\n",
    "dropout_2 = Dropout(0.5)(flatten_2)\n",
    "output_2 = Dense(units=12, activation='softmax')(dropout_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22ae5e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 142)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 142, 200)     2000200     ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 142, 200, 1)  0           ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 140, 1, 100)  60100       ['reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 139, 1, 100)  80100       ['reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 138, 1, 100)  100100      ['reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 1, 1, 100)   0           ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_4 (MaxPooling2D)  (None, 1, 1, 100)   0           ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_5 (MaxPooling2D)  (None, 1, 1, 100)   0           ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 3, 1, 100)    0           ['max_pooling2d_3[0][0]',        \n",
      "                                                                  'max_pooling2d_4[0][0]',        \n",
      "                                                                  'max_pooling2d_5[0][0]']        \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 300)          0           ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 300)          0           ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 12)           3612        ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,244,112\n",
      "Trainable params: 243,912\n",
      "Non-trainable params: 2,000,200\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_2 = Model(inputs=inputs_2, outputs=output_2)\n",
    "model_2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "419ac96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "192/192 [==============================] - 3s 13ms/step - loss: 2.7309 - accuracy: 0.5569 - val_loss: 2.6208 - val_accuracy: 0.3885\n",
      "Epoch 2/5\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 2.1776 - accuracy: 0.6806 - val_loss: 2.6541 - val_accuracy: 0.2636\n",
      "Epoch 3/5\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 2.0480 - accuracy: 0.7321 - val_loss: 2.3716 - val_accuracy: 0.4929\n",
      "Epoch 4/5\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 1.9610 - accuracy: 0.7634 - val_loss: 2.5133 - val_accuracy: 0.4745\n",
      "Epoch 5/5\n",
      "192/192 [==============================] - 2s 8ms/step - loss: 1.9005 - accuracy: 0.7776 - val_loss: 2.6465 - val_accuracy: 0.3033\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "history_2 = model_2.fit(X_train, y_train, epochs=5, batch_size=batch_size, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b1a1ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_2 = model_2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2327c95c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29896907216494845"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a15a505c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  0,  0,  0,  0,  0,  0,  0,  0, 17,  0,  0],\n",
       "       [ 0, 51,  0,  0,  0,  0,  0,  0,  0,  6,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0, 25,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0, 46,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0, 96,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  1,  0, 69,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0, 17,  0,  0, 26,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 42,  0,  1,  0,  0],\n",
       "       [ 1,  0,  0,  0,  0,  0,  0,  0,  0, 23,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  6,  0, 46,  0,  2],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0, 67,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0, 22,  0, 17]], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat_2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a7bb35",
   "metadata": {},
   "source": [
    "# Model 3: w2v with trainable embeddings\n",
    "\n",
    "For this model we're going to try the same model again, but this time make the embeddings trainable. That means if during training the model decides on a better embedding for a word then it'll update it\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ebf742bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_3 = Input(shape=(sequence_length,), dtype='int32')\n",
    "embedding_layer_3 = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=sequence_length,\n",
    "                            trainable=True)(inputs_3)\n",
    "\n",
    "reshape_3 = Reshape((sequence_length, embedding_dim, 1))(embedding_layer_3)\n",
    "\n",
    "# note the relu activation\n",
    "conv_0_3 = Conv2D(num_filters, kernel_size=(3, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_3)\n",
    "conv_1_3 = Conv2D(num_filters, kernel_size=(4, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_3)\n",
    "conv_2_3 = Conv2D(num_filters, kernel_size=(5, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_3)\n",
    "\n",
    "maxpool_0_3 = MaxPool2D(pool_size=(sequence_length - 3 + 1, 1), strides=(1,1), padding='valid')(conv_0_3)\n",
    "maxpool_1_3 = MaxPool2D(pool_size=(sequence_length - 4 + 1, 1), strides=(1,1), padding='valid')(conv_1_3)\n",
    "maxpool_2_3 = MaxPool2D(pool_size=(sequence_length - 5 + 1, 1), strides=(1,1), padding='valid')(conv_2_3)\n",
    "\n",
    "concatenated_tensor_3 = Concatenate(axis=1)([maxpool_0_3, maxpool_1_3, maxpool_2_3])\n",
    "flatten_3 = Flatten()(concatenated_tensor_3)\n",
    "\n",
    "dropout_3 = Dropout(0.5)(flatten_3)\n",
    "output_3 = Dense(units=12, activation='softmax')(dropout_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "087cbcbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 142)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, 142, 200)     2000200     ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)            (None, 142, 200, 1)  0           ['embedding_2[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 140, 1, 100)  60100       ['reshape_2[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 139, 1, 100)  80100       ['reshape_2[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 138, 1, 100)  100100      ['reshape_2[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_6 (MaxPooling2D)  (None, 1, 1, 100)   0           ['conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_7 (MaxPooling2D)  (None, 1, 1, 100)   0           ['conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_8 (MaxPooling2D)  (None, 1, 1, 100)   0           ['conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 3, 1, 100)    0           ['max_pooling2d_6[0][0]',        \n",
      "                                                                  'max_pooling2d_7[0][0]',        \n",
      "                                                                  'max_pooling2d_8[0][0]']        \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 300)          0           ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 300)          0           ['flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 12)           3612        ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,244,112\n",
      "Trainable params: 2,244,112\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_3 = Model(inputs=inputs_3, outputs=output_3)\n",
    "model_3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "666fc00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "192/192 [==============================] - 8s 40ms/step - loss: 2.6191 - accuracy: 0.6188 - val_loss: 2.5045 - val_accuracy: 0.5464\n",
      "Epoch 2/5\n",
      "192/192 [==============================] - 2s 12ms/step - loss: 1.7412 - accuracy: 0.8068 - val_loss: 1.9863 - val_accuracy: 0.7109\n",
      "Epoch 3/5\n",
      "192/192 [==============================] - 2s 12ms/step - loss: 1.4804 - accuracy: 0.8597 - val_loss: 1.6798 - val_accuracy: 0.7059\n",
      "Epoch 4/5\n",
      "192/192 [==============================] - 2s 12ms/step - loss: 1.3336 - accuracy: 0.8827 - val_loss: 1.6826 - val_accuracy: 0.7761\n",
      "Epoch 5/5\n",
      "192/192 [==============================] - 2s 12ms/step - loss: 1.2143 - accuracy: 0.8965 - val_loss: 1.5227 - val_accuracy: 0.8367\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "history_3 = model_3.fit(X_train, y_train, epochs=5, batch_size=batch_size, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f37ffd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_3 = model_3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7558c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5137457044673539"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat_3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1710ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3,  0,  0,  0,  0,  0,  0,  0,  8,  7,  0,  0],\n",
       "       [ 0, 56,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0],\n",
       "       [ 0,  0, 19,  0,  0,  0,  0,  0,  0,  6,  0,  0],\n",
       "       [ 0,  0,  0,  0,  2,  0,  0,  0,  0, 44,  0,  0],\n",
       "       [ 0,  0,  2,  0, 20,  0,  0,  0,  0, 49,  3, 22],\n",
       "       [ 0,  0,  0,  0,  0, 38,  0,  1,  0, 31,  0,  0],\n",
       "       [ 0,  0,  0,  0,  2, 21,  0,  0,  0, 20,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 42,  0,  0,  1,  0],\n",
       "       [ 1,  0,  0,  0,  0,  0,  0,  0,  9, 14,  0,  0],\n",
       "       [ 0,  0,  1,  0,  0,  0,  0,  6,  0, 13,  0, 34],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  5, 62,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  2,  0, 37]], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat_3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c034668a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN random       : 0.520618556701031\n",
      "CNN static       : 0.29896907216494845\n",
      "CNN trainable    : 0.5137457044673539\n"
     ]
    }
   ],
   "source": [
    "print(\"CNN random       : \" + str(accuracy_score(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat)))))\n",
    "print(\"CNN static       : \" + str(accuracy_score(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat_2)))))\n",
    "print(\"CNN trainable    : \" + str(accuracy_score(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat_3)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88d6a60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch] *",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
