{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cbcf794c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "# lets import some stuff\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import Constant\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re\n",
    "from keras import regularizers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "925ae8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2060\n"
     ]
    }
   ],
   "source": [
    "# confirm  the GPU\n",
    "from torch import cuda\n",
    "assert cuda.is_available()\n",
    "assert cuda.device_count() > 0\n",
    "print(cuda.get_device_name(cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aa2b5390",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000 # this is the number of words we care about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "80dfff66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>section_nr</th>\n",
       "      <th>has_citation</th>\n",
       "      <th>last_section_title</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This live defined as 1 00 Defination</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Defination</td>\n",
       "      <td>Defination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A sunshine be a maven that shine 1 00 Defination</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Defination</td>\n",
       "      <td>Defination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e set galaxy as the group of star 1 00 Defination</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Defination</td>\n",
       "      <td>Defination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e ask that these were in all likeliness stimul...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Hypothesis</td>\n",
       "      <td>Hypothesis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Information technology could trace theorise th...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Hypothesis</td>\n",
       "      <td>Hypothesis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18147</th>\n",
       "      <td>we therefore conclude that the issue of experi...</td>\n",
       "      <td>70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>functional magnetic resonance imaging data ana...</td>\n",
       "      <td>Emperical Result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18149</th>\n",
       "      <td>these new insight we discovered open possiblen...</td>\n",
       "      <td>80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>contributions to research</td>\n",
       "      <td>Future work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18153</th>\n",
       "      <td>as users tend to behave impulsively with mobil...</td>\n",
       "      <td>87</td>\n",
       "      <td>1.0</td>\n",
       "      <td>limitations and future topics</td>\n",
       "      <td>limitation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18154</th>\n",
       "      <td>tertiary using yes  no resolution choice for e...</td>\n",
       "      <td>87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>limitations and future topics</td>\n",
       "      <td>limitation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18155</th>\n",
       "      <td>fifth our report centre on highly negative inc...</td>\n",
       "      <td>87</td>\n",
       "      <td>1.0</td>\n",
       "      <td>limitations and future topics</td>\n",
       "      <td>limitation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11966 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  section_nr  \\\n",
       "0                   This live defined as 1 00 Defination           1   \n",
       "1       A sunshine be a maven that shine 1 00 Defination           1   \n",
       "2      e set galaxy as the group of star 1 00 Defination           1   \n",
       "3      e ask that these were in all likeliness stimul...           1   \n",
       "4      Information technology could trace theorise th...           4   \n",
       "...                                                  ...         ...   \n",
       "18147  we therefore conclude that the issue of experi...          70   \n",
       "18149  these new insight we discovered open possiblen...          80   \n",
       "18153  as users tend to behave impulsively with mobil...          87   \n",
       "18154  tertiary using yes  no resolution choice for e...          87   \n",
       "18155  fifth our report centre on highly negative inc...          87   \n",
       "\n",
       "       has_citation                                 last_section_title  \\\n",
       "0               0.0                                         Defination   \n",
       "1               0.0                                         Defination   \n",
       "2               0.0                                         Defination   \n",
       "3               0.0                                         Hypothesis   \n",
       "4               0.0                                         Hypothesis   \n",
       "...             ...                                                ...   \n",
       "18147           0.0  functional magnetic resonance imaging data ana...   \n",
       "18149           1.0                         contributions to research    \n",
       "18153           1.0                     limitations and future topics    \n",
       "18154           0.0                     limitations and future topics    \n",
       "18155           1.0                     limitations and future topics    \n",
       "\n",
       "                 Labels  \n",
       "0            Defination  \n",
       "1            Defination  \n",
       "2            Defination  \n",
       "3            Hypothesis  \n",
       "4            Hypothesis  \n",
       "...                 ...  \n",
       "18147  Emperical Result  \n",
       "18149       Future work  \n",
       "18153        limitation  \n",
       "18154        limitation  \n",
       "18155        limitation  \n",
       "\n",
       "[11966 rows x 5 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read training dataset without rule based feature\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\nitis\\Sentence classification\\theis_final\\preprocessed_train.csv', index_col=0)\n",
    "df = df.drop_duplicates(subset='sentence', keep=\"first\")\n",
    "#df = df[1:100]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1f7c6a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean length of sentence: 27.707003175664383\n",
      "max length of sentence: 141\n",
      "std dev length of sentence: 16.051585128406025\n"
     ]
    }
   ],
   "source": [
    "df['l'] = df['sentence'].apply(lambda x: len(str(x).split(' ')))\n",
    "print(\"mean length of sentence: \" + str(df.l.mean()))\n",
    "print(\"max length of sentence: \" + str(df.l.max()))\n",
    "print(\"std dev length of sentence: \" + str(df.l.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "103d7c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 141"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6fd70a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>section_nr</th>\n",
       "      <th>has_citation</th>\n",
       "      <th>last_section_title</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The aim of this thesis was to gain an understa...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Research Aim</td>\n",
       "      <td>Aim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The aim is to develop finite element models us...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Research Objective</td>\n",
       "      <td>Aim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Determine the relationship between the size of...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Research Objective</td>\n",
       "      <td>Aim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Investigate the influence of nonuniform cup su...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Research Objective</td>\n",
       "      <td>Aim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Examine the influence of errors during reaming...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Research Objective</td>\n",
       "      <td>Aim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>the moment of enjoyment on the pattern to rais...</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>hypothesis outcome</td>\n",
       "      <td>Hypothesis Result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>Information technology substantiate the hypoth...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>hypothesis outcome</td>\n",
       "      <td>Hypothesis Result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>information engineering science rejects the su...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>hypothesis outcome</td>\n",
       "      <td>Hypothesis Result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>the supposition stern follow agreed on the cor...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>hypothesis outcome</td>\n",
       "      <td>Hypothesis Result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>the final result prove the possibility that th...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>hypothesis outcome</td>\n",
       "      <td>Hypothesis Result</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>582 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence  section_nr  \\\n",
       "0    The aim of this thesis was to gain an understa...           1   \n",
       "1    The aim is to develop finite element models us...           1   \n",
       "2    Determine the relationship between the size of...           1   \n",
       "3    Investigate the influence of nonuniform cup su...           1   \n",
       "4    Examine the influence of errors during reaming...           1   \n",
       "..                                                 ...         ...   \n",
       "577  the moment of enjoyment on the pattern to rais...          39   \n",
       "578  Information technology substantiate the hypoth...          10   \n",
       "579  information engineering science rejects the su...          10   \n",
       "580  the supposition stern follow agreed on the cor...          10   \n",
       "581  the final result prove the possibility that th...          10   \n",
       "\n",
       "     has_citation   last_section_title             Labels  \n",
       "0               0         Research Aim                Aim  \n",
       "1               0   Research Objective                Aim  \n",
       "2               0   Research Objective                Aim  \n",
       "3               0   Research Objective                Aim  \n",
       "4               0   Research Objective                Aim  \n",
       "..            ...                  ...                ...  \n",
       "577             0  hypothesis outcome   Hypothesis Result  \n",
       "578             0  hypothesis outcome   Hypothesis Result  \n",
       "579             0  hypothesis outcome   Hypothesis Result  \n",
       "580             0  hypothesis outcome   Hypothesis Result  \n",
       "581             0  hypothesis outcome   Hypothesis Result  \n",
       "\n",
       "[582 rows x 5 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get test data without rule based features\n",
    "cols = ['sentence', 'section_nr','has_citation','last_section_title','Labels']\n",
    "cols1 = ['sentence', 'section_nr','has_citation','last_section_title']\n",
    "test = pd.read_csv(r'C:\\Users\\nitis\\Sentence classification\\theis_final\\preprocessed_test.csv')\n",
    "test = test[cols]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "37723b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train = df[\"sentence\"].values\n",
    "sentences_test = test[\"sentence\"].values\n",
    "\n",
    "y_train =  pd.get_dummies(df['Labels']).values\n",
    "y_test  = pd.get_dummies(test['Labels']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7bc10407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e set galaxy as the group of star 1 00 Defination\n",
      "[168, 208, 4201, 15, 1, 247, 3, 1230, 9, 2, 1945]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(sentences_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
    "X_test = tokenizer.texts_to_sequences(sentences_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "\n",
    "print(sentences_train[2])\n",
    "print(X_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "00723eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  16  345  533   15    9    2 1945    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0]\n"
     ]
    }
   ],
   "source": [
    "X_train = pad_sequences(X_train, padding='post', maxlen=sequence_length)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=sequence_length)\n",
    "\n",
    "print(X_train[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b3c34b",
   "metadata": {},
   "source": [
    "#  Model 1: Random embeddings\n",
    "\n",
    "Lets build our model. In general I'm going to just use the same hyperparameters as Kim does apart from the embedding dimension\n",
    "\n",
    "Keras has an Embedding layer we can use here. If you don't specify a custom way to embed text (something we will do later with w2v) Keras will do it randomly with a normal (Gaussian) distribution for you\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1a3f0e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 200 # Kim uses 300 here\n",
    "num_filters = 100\n",
    "\n",
    "inputs = Input(shape=(sequence_length,), dtype='int32')\n",
    "\n",
    "# use a random embedding for the text\n",
    "embedding_layer = Embedding(input_dim=max_features, output_dim=embedding_dim, input_length=sequence_length)(inputs)\n",
    "\n",
    "reshape = Reshape((sequence_length, embedding_dim, 1))(embedding_layer)\n",
    "\n",
    "# Note the relu activation which Kim specifically mentions\n",
    "# He also uses an l2 constraint of 3\n",
    "# Also, note that the convolution window acts on the whole 200 dimensions - that's important\n",
    "conv_0 = Conv2D(num_filters, kernel_size=(3, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape)\n",
    "conv_1 = Conv2D(num_filters, kernel_size=(4, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape)\n",
    "conv_2 = Conv2D(num_filters, kernel_size=(5, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape)\n",
    "\n",
    "# perform max pooling on each of the convoluations\n",
    "maxpool_0 = MaxPool2D(pool_size=(sequence_length - 3 + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
    "maxpool_1 = MaxPool2D(pool_size=(sequence_length - 4 + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
    "maxpool_2 = MaxPool2D(pool_size=(sequence_length - 5 + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
    "\n",
    "# concat and flatten\n",
    "concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
    "flatten = Flatten()(concatenated_tensor)\n",
    "\n",
    "# do dropout and predict\n",
    "dropout = Dropout(0.5)(flatten)\n",
    "output = Dense(units=12, activation='softmax')(dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f93b6f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 141)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, 141, 200)     2000000     ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)            (None, 141, 200, 1)  0           ['embedding_2[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 139, 1, 100)  60100       ['reshape_2[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 138, 1, 100)  80100       ['reshape_2[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 137, 1, 100)  100100      ['reshape_2[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_6 (MaxPooling2D)  (None, 1, 1, 100)   0           ['conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_7 (MaxPooling2D)  (None, 1, 1, 100)   0           ['conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_8 (MaxPooling2D)  (None, 1, 1, 100)   0           ['conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 3, 1, 100)    0           ['max_pooling2d_6[0][0]',        \n",
      "                                                                  'max_pooling2d_7[0][0]',        \n",
      "                                                                  'max_pooling2d_8[0][0]']        \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 300)          0           ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 300)          0           ['flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 12)           3612        ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,243,912\n",
      "Trainable params: 2,243,912\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=inputs, outputs=output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "65bd8e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "216/216 [==============================] - 3s 12ms/step - loss: 2.7974 - accuracy: 0.1185 - val_loss: 2.4972 - val_accuracy: 0.2322\n",
      "Epoch 2/5\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 2.1669 - accuracy: 0.3289 - val_loss: 2.1552 - val_accuracy: 0.2865\n",
      "Epoch 3/5\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 1.8859 - accuracy: 0.4846 - val_loss: 1.9752 - val_accuracy: 0.3141\n",
      "Epoch 4/5\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 1.6750 - accuracy: 0.5827 - val_loss: 1.8638 - val_accuracy: 0.4770\n",
      "Epoch 5/5\n",
      "216/216 [==============================] - 2s 12ms/step - loss: 1.5489 - accuracy: 0.6468 - val_loss: 1.8913 - val_accuracy: 0.4503\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50 # Kim uses 50 here, I have a slightly smaller sample size than num\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=batch_size, verbose=1, validation_split=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7ed2dd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8c759e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4725085910652921"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3b341bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0, 12,  0,  0,  0,  0,  0,  0,  0,  6,  0],\n",
       "       [ 0, 56,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0],\n",
       "       [ 0,  3,  3,  0,  0,  0,  0,  0,  0,  0, 19,  0],\n",
       "       [ 0,  0,  5, 30,  5,  0,  0,  3,  0,  1,  0,  2],\n",
       "       [ 0,  5, 28,  3, 42,  1,  0,  2,  0,  0,  1, 14],\n",
       "       [ 0,  0,  0,  2,  5, 38,  0,  1,  0,  0, 24,  0],\n",
       "       [ 0,  0,  0,  2, 10, 31,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  1,  4,  0,  0,  0,  0, 37,  0,  0,  1,  0],\n",
       "       [ 0,  0,  2,  3,  0,  0,  0,  0,  0,  0, 19,  0],\n",
       "       [ 0,  0, 34,  2,  0,  0,  0,  6,  0,  0, 12,  0],\n",
       "       [ 0,  0,  0,  0,  2,  0,  0,  0,  0,  0, 65,  0],\n",
       "       [ 0,  1,  5,  0, 22,  3,  0,  3,  0,  0,  1,  4]], dtype=int64)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6125d3",
   "metadata": {},
   "source": [
    "#  Model 2: Static word2vec\n",
    "Now rather than randomly assign vectors we're going use w2v embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f1af0bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join('C:\\\\Users\\\\nitis\\\\OneDrive\\\\Desktop\\\\thesis', 'glove.6B.200d.txt'), encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "74a77f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10958 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2b5c9222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10001\n"
     ]
    }
   ],
   "source": [
    "num_words = min(max_features, len(word_index)) + 1\n",
    "print(num_words)\n",
    "\n",
    "# first create a matrix of zeros, this is our embedding matrix\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "# for each word in out tokenizer lets try to find that work in our w2v model\n",
    "for word, i in word_index.items():\n",
    "    if i > max_features:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # we found the word - add that words vector to the matrix\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        # doesn't exist, assign a random vector\n",
    "        embedding_matrix[i] = np.random.randn(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f9809347",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_2 = Input(shape=(sequence_length,), dtype='int32')\n",
    "\n",
    "# note the `trainable=False`, later we will make this layer trainable\n",
    "embedding_layer_2 = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=sequence_length,\n",
    "                            trainable=False)(inputs_2)\n",
    "\n",
    "reshape_2 = Reshape((sequence_length, embedding_dim, 1))(embedding_layer_2)\n",
    "\n",
    "conv_0_2 = Conv2D(num_filters, kernel_size=(3, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_2)\n",
    "conv_1_2 = Conv2D(num_filters, kernel_size=(4, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_2)\n",
    "conv_2_2 = Conv2D(num_filters, kernel_size=(5, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_2)\n",
    "\n",
    "maxpool_0_2 = MaxPool2D(pool_size=(sequence_length - 3 + 1, 1), strides=(1,1), padding='valid')(conv_0_2)\n",
    "maxpool_1_2 = MaxPool2D(pool_size=(sequence_length - 4 + 1, 1), strides=(1,1), padding='valid')(conv_1_2)\n",
    "maxpool_2_2 = MaxPool2D(pool_size=(sequence_length - 5 + 1, 1), strides=(1,1), padding='valid')(conv_2_2)\n",
    "\n",
    "concatenated_tensor_2 = Concatenate(axis=1)([maxpool_0_2, maxpool_1_2, maxpool_2_2])\n",
    "flatten_2 = Flatten()(concatenated_tensor_2)\n",
    "\n",
    "dropout_2 = Dropout(0.5)(flatten_2)\n",
    "output_2 = Dense(units=12, activation='softmax')(dropout_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "22ae5e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 141)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_4 (Embedding)        (None, 141, 200)     2000200     ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " reshape_4 (Reshape)            (None, 141, 200, 1)  0           ['embedding_4[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 139, 1, 100)  60100       ['reshape_4[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 138, 1, 100)  80100       ['reshape_4[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 137, 1, 100)  100100      ['reshape_4[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_12 (MaxPooling2D  (None, 1, 1, 100)   0           ['conv2d_12[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_13 (MaxPooling2D  (None, 1, 1, 100)   0           ['conv2d_13[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_14 (MaxPooling2D  (None, 1, 1, 100)   0           ['conv2d_14[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 3, 1, 100)    0           ['max_pooling2d_12[0][0]',       \n",
      "                                                                  'max_pooling2d_13[0][0]',       \n",
      "                                                                  'max_pooling2d_14[0][0]']       \n",
      "                                                                                                  \n",
      " flatten_4 (Flatten)            (None, 300)          0           ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 300)          0           ['flatten_4[0][0]']              \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 12)           3612        ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,244,112\n",
      "Trainable params: 243,912\n",
      "Non-trainable params: 2,000,200\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_2 = Model(inputs=inputs_2, outputs=output_2)\n",
    "model_2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "419ac96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 2.8541 - accuracy: 0.3786 - val_loss: 3.1967 - val_accuracy: 0.1061\n",
      "Epoch 2/5\n",
      "192/192 [==============================] - 2s 8ms/step - loss: 2.4100 - accuracy: 0.4840 - val_loss: 3.0628 - val_accuracy: 0.1170\n",
      "Epoch 3/5\n",
      "192/192 [==============================] - 2s 8ms/step - loss: 2.3597 - accuracy: 0.5354 - val_loss: 3.1294 - val_accuracy: 0.1763\n",
      "Epoch 4/5\n",
      "192/192 [==============================] - 2s 8ms/step - loss: 2.2940 - accuracy: 0.5873 - val_loss: 2.8388 - val_accuracy: 0.3133\n",
      "Epoch 5/5\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 2.2084 - accuracy: 0.6334 - val_loss: 2.7831 - val_accuracy: 0.2794\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "history_2 = model_2.fit(X_train, y_train, epochs=5, batch_size=batch_size, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4b1a1ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_2 = model_2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2327c95c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13745704467353953"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a15a505c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6,  0,  0,  0,  0,  0,  0,  0,  0, 12,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0, 57,  0,  0],\n",
       "       [ 9,  0,  0,  0,  0,  0,  0,  0,  0, 16,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0, 46,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0, 96,  0,  0],\n",
       "       [ 2,  0,  0,  0,  0,  4,  0,  0,  0, 64,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  2,  0,  0,  0, 41,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 19,  0, 24,  0,  0],\n",
       "       [ 8,  0,  0,  0,  0,  0,  0,  0,  0, 16,  0,  0],\n",
       "       [ 3,  0,  0,  0,  0,  0,  0,  0,  0, 51,  0,  0],\n",
       "       [ 5,  0,  0,  0,  0,  0,  0,  0,  0, 62,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0, 39,  0,  0]], dtype=int64)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat_2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a7bb35",
   "metadata": {},
   "source": [
    "# Model 3: w2v with trainable embeddings\n",
    "\n",
    "For this model we're going to try the same model again, but this time make the embeddings trainable. That means if during training the model decides on a better embedding for a word then it'll update it\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ebf742bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_3 = Input(shape=(sequence_length,), dtype='int32')\n",
    "embedding_layer_3 = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=sequence_length,\n",
    "                            trainable=True)(inputs_3)\n",
    "\n",
    "reshape_3 = Reshape((sequence_length, embedding_dim, 1))(embedding_layer_3)\n",
    "\n",
    "# note the relu activation\n",
    "conv_0_3 = Conv2D(num_filters, kernel_size=(3, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_3)\n",
    "conv_1_3 = Conv2D(num_filters, kernel_size=(4, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_3)\n",
    "conv_2_3 = Conv2D(num_filters, kernel_size=(5, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_3)\n",
    "\n",
    "maxpool_0_3 = MaxPool2D(pool_size=(sequence_length - 3 + 1, 1), strides=(1,1), padding='valid')(conv_0_3)\n",
    "maxpool_1_3 = MaxPool2D(pool_size=(sequence_length - 4 + 1, 1), strides=(1,1), padding='valid')(conv_1_3)\n",
    "maxpool_2_3 = MaxPool2D(pool_size=(sequence_length - 5 + 1, 1), strides=(1,1), padding='valid')(conv_2_3)\n",
    "\n",
    "concatenated_tensor_3 = Concatenate(axis=1)([maxpool_0_3, maxpool_1_3, maxpool_2_3])\n",
    "flatten_3 = Flatten()(concatenated_tensor_3)\n",
    "\n",
    "dropout_3 = Dropout(0.5)(flatten_3)\n",
    "output_3 = Dense(units=12, activation='softmax')(dropout_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "087cbcbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)           [(None, 141)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_5 (Embedding)        (None, 141, 200)     2000200     ['input_6[0][0]']                \n",
      "                                                                                                  \n",
      " reshape_5 (Reshape)            (None, 141, 200, 1)  0           ['embedding_5[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 139, 1, 100)  60100       ['reshape_5[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 138, 1, 100)  80100       ['reshape_5[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 137, 1, 100)  100100      ['reshape_5[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_15 (MaxPooling2D  (None, 1, 1, 100)   0           ['conv2d_15[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_16 (MaxPooling2D  (None, 1, 1, 100)   0           ['conv2d_16[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_17 (MaxPooling2D  (None, 1, 1, 100)   0           ['conv2d_17[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 3, 1, 100)    0           ['max_pooling2d_15[0][0]',       \n",
      "                                                                  'max_pooling2d_16[0][0]',       \n",
      "                                                                  'max_pooling2d_17[0][0]']       \n",
      "                                                                                                  \n",
      " flatten_5 (Flatten)            (None, 300)          0           ['concatenate_5[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 300)          0           ['flatten_5[0][0]']              \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 12)           3612        ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,244,112\n",
      "Trainable params: 2,244,112\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_3 = Model(inputs=inputs_3, outputs=output_3)\n",
    "model_3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "666fc00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "192/192 [==============================] - 8s 39ms/step - loss: 2.7890 - accuracy: 0.4520 - val_loss: 2.9616 - val_accuracy: 0.1399\n",
      "Epoch 2/5\n",
      "192/192 [==============================] - 2s 12ms/step - loss: 2.0038 - accuracy: 0.7042 - val_loss: 2.4094 - val_accuracy: 0.4311\n",
      "Epoch 3/5\n",
      "192/192 [==============================] - 2s 12ms/step - loss: 1.6110 - accuracy: 0.8094 - val_loss: 2.0077 - val_accuracy: 0.5038\n",
      "Epoch 4/5\n",
      "192/192 [==============================] - 2s 12ms/step - loss: 1.4183 - accuracy: 0.8429 - val_loss: 1.7836 - val_accuracy: 0.6107\n",
      "Epoch 5/5\n",
      "192/192 [==============================] - 2s 12ms/step - loss: 1.2831 - accuracy: 0.8691 - val_loss: 1.7630 - val_accuracy: 0.6541\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 50\n",
    "history_3 = model_3.fit(X_train, y_train, epochs=5, batch_size=batch_size, verbose=1, validation_split=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5f37ffd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_3 = model_3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f7558c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48625429553264604"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat_3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c1710ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6,  0,  0,  0,  0,  0,  0,  0,  0,  2, 10,  0],\n",
       "       [ 0, 56,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0],\n",
       "       [11,  3,  3,  0,  0,  0,  0,  0,  0,  4,  4,  0],\n",
       "       [ 0,  0,  0,  0,  2,  0,  0,  3,  0, 41,  0,  0],\n",
       "       [ 0,  3,  0,  0, 44,  0,  0,  0,  0,  3, 23, 23],\n",
       "       [19,  0,  0,  0,  0, 35,  0,  2,  0,  9,  5,  0],\n",
       "       [ 0,  0,  0,  0,  1, 30,  0,  0,  0, 12,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 39,  0,  3,  1,  0],\n",
       "       [18,  0,  0,  0,  0,  0,  0,  0,  0,  0,  6,  0],\n",
       "       [ 3,  0,  0,  0,  0,  0,  0,  6,  0,  2,  9, 34],\n",
       "       [ 6,  0,  0,  0,  0,  0,  0,  0,  0,  2, 59,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 39]], dtype=int64)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat_3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c034668a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN random       : 0.4725085910652921\n",
      "CNN static       : 0.13745704467353953\n",
      "CNN trainable    : 0.48625429553264604\n"
     ]
    }
   ],
   "source": [
    "print(\"CNN random       : \" + str(accuracy_score(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat)))))\n",
    "print(\"CNN static       : \" + str(accuracy_score(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat_2)))))\n",
    "print(\"CNN trainable    : \" + str(accuracy_score(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat_3)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88d6a60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch] *",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
